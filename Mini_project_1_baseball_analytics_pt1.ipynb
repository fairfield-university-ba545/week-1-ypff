{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Project #1: Baseball Analytics\n",
    "\n",
    "The overall purpose of this mini-project is to predicting MLB wins per season by modeling data to KMeans clustering model and linear regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Import and Handling\n",
    "\n",
    "In this tutorial, you’ll see how you can easily load in data from a database with `sqlite3`, how you can explore your data and improve its data quality with pandas and matplotlib, and how you can then use the `Scikit-Learn` package to extract some valid insights out of your data.\n",
    "\n",
    "You will read in the data by querying a `sqlite` database using the sqlite3 package and converting to a DataFrame with pandas. Your data will be filtered to only include currently active modern teams and only years where the team played `150` or more games.\n",
    "\n",
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing pandas and sqlite3 packages\n",
    "import pandas as pd\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#connecting the sqlite database to the notebook\n",
    "conn = sqlite3.connect(\"../data/lahman2016.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Writing the SQL query to select the teams which have played more than 150 games and are still active\n",
    "query = '''select * from Teams\n",
    "inner join TeamsFranchises on Teams.franchID == TeamsFranchises.franchID\n",
    "where Teams.G >= 150 and TeamsFranchises.active== 'Y';  '''\n",
    "\n",
    "\n",
    "#Executing the query \n",
    "Teams = conn.execute(query).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting the result into a Pandas dataframe \n",
    "#### Complete the code block below\n",
    "#### create a df called `df_Teams' and from the data `Teams`\n",
    "\n",
    "#### display the first 10 rows of `df_Teams`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the standard process of reading data from a database (`sqlite`) to a `pandas` dataframe. \n",
    "\n",
    "Please note that it is a common practice reading data from enterprise systems - so please familiarize yourselves with this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Prior to assessing the data quality, let’s first eliminate the columns that aren’t necessary or are derived from the target column (`Wins`). This is where knowledge of the data you are working with starts to become very valuable.\n",
    "\n",
    "You should have noted that our dataframe (`df_Teams`) has no column names. Let's first define the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding column names to dataframe\n",
    "cols = ['yearID','lgID','teamID','franchID','divID','Rank','G','Ghome','W','L','DivWin','WCWin','LgWin','WSWin',\n",
    "        'R','AB','H','2B','3B','HR','BB','SO','SB','CS','HBP','SF','RA','ER','ERA','CG','SHO','SV','IPouts','HA',\n",
    "        'HRA','BBA','SOA','E','DP','FP','name','park','attendance','BPF','PPF','teamIDBR','teamIDlahman45',\n",
    "        'teamIDretro','franchID','franchName','active','NAassoc']\n",
    "\n",
    "#### Complete your code below\n",
    "#### define columns of `df_Teams` to `cols`\n",
    "\n",
    "\n",
    "#### display the first 5 rows of `df_Teams` \n",
    "#### you should see 52 columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the columns are not as useful as the others. So that we are going to delete them.\n",
    "\n",
    "__Note__: what we are doing right now is part of __feature selection__, we have multiple ways of feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropping your unnecesary column variables.\n",
    "drop_cols = ['lgID','franchID','divID','Rank','Ghome','L','DivWin','WCWin','LgWin','WSWin','SF','name','park',\n",
    "             'attendance','BPF','PPF','teamIDBR','teamIDlahman45','teamIDretro','franchID','franchName','active',\n",
    "             'NAassoc']\n",
    "\n",
    "#### drop columns contained in `drop_cols` from `df_Teams`\n",
    "#### and save the remaining columns as `df`\n",
    "\n",
    "\n",
    "#### make sure that these columns are deleted by looking at the first 5 rows of `df`\n",
    "#### you should see only 29 columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values\n",
    "\n",
    "`2` of the columns have a relatively small amount of null values. There are `110` null values in the `SO` (Strike Outs) column and `22` in the DP (Double Play) column. Two of the columns have a relatively large amount of them. There are `419` null values in the `CS` (Caught Stealing) column and `1777` in the `HBP` (Hit by Pitch) column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Identifying the number of null values in the dataframe\n",
    "# Creating a for loop to display the column names and also their count of missing values \n",
    "\n",
    "#### complete the code below\n",
    "#### create an empty list named `names` for columns names\n",
    "\n",
    "#### create an empty list named `val` for # of null values in each column\n",
    "\n",
    "\n",
    "#### create a for loop iterating each `col` through `df.columns`\n",
    "\n",
    "    #### add column name `col` to `names`\n",
    "    \n",
    "    #### add # of null values to `val`\n",
    "    #### you can get # of null values for column `c` as `df[c].isnull().sum()`\n",
    "     \n",
    "    #### print out results as (column_name, # of null values in column_name)\n",
    "    #### Note that `col` is the current column_name in iteration\n",
    "    #### and you should retrieve the # of null values in column_name as the last element in `val`\n",
    "    #### hint: the last element in a list l is: l[-1]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to drop two columns (`CS` and `HBP`) with too many missing values.\n",
    "\n",
    "__NOTE__: even though we said that dropping columns with missing values is the __last resort__, the reason we are dropping the columns here is that because of the number of missing values, it will be very difficult for us to impute them in these two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dropping the columns with large number of null values\n",
    "\n",
    "\n",
    "#### drop `CS` & `HBP` from `df`\n",
    "#### and save the remaining as `df`\n",
    "\n",
    "\n",
    "#### check the first 5 rows of the new `df` to see \n",
    "#### if the two columns are successfully dropped\n",
    "#### you should expect to see 27 columns now\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the two columns dropped, we can impute the missing values in the other two columns (`SO` and `DP`) since they have much less.\n",
    "\n",
    "### Impute Missing Values\n",
    "\n",
    "As a design decision, we decide to use `median` instead of `mean` to impute the missing values. Use the block below to answer __why we made that decision__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Double click and type your answer here__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filling the missing values with the median of their respective column values\n",
    "#### complete your code below\n",
    "#### you can impute the missing values in a column using the `fillna()` function provided by `pandas`\n",
    "#### fillna() works as `df[col].fillna(some_value)`, in this case the value is `df[col].median()`\n",
    "#### NOTE that `col` above is the name of the column you want to impute\n",
    "\n",
    "\n",
    "\n",
    "#### Double-check if the null values are filled\n",
    "#### you should see `0`s for all columns \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an easier way of checking null values in a dataframe. You can use either way based on your preference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Again checking for null values \n",
    "# if see any value which is not `0` that means you still have null values in your data\n",
    "# in this case we are fine\n",
    "print(df.isnull().sum(axis=0).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring and Visualizing The Data\n",
    "\n",
    "Exploring your data using different types of visualizations is always a good practice when doing EDA.\n",
    "\n",
    "You’ll start by plotting a histogram of the target column (`W`) so you can see the distribution of wins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#### the statement below ask matplotlib to use the 'ggplot' style\n",
    "#### you should consider using that\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Complete your code below\n",
    "#### create a histogram `hist()` over the column `df['W']`\n",
    "\n",
    "\n",
    "#### adding elements to your visualization to increase the readability\n",
    "#### you should always have title and axis name(s) in your visualization\n",
    "#### name your x-axis label as `Wins`\n",
    "\n",
    "\n",
    "#### name your visualization title as `Distribution of Wins`\n",
    "\n",
    "\n",
    "#### show your visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### We can also check the descriptive stats of `df['W']` using `.describe()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should already know that `df['W']` is a __continuous__ field. \n",
    "\n",
    "__Answer this question__: When the target variable is continuous, what type of analysis are we doing (select from classification/clustering/regression), and why? __Use the block below to answer__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Double click and type your answer__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to convert a continuous variable to categorical is through binning. You can bin your continuous variable using its distribution, any external knowledge, or some other logical reasons. But __be sure to include your reason in your analytical report__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating Bins for ploting and understanding of the target \n",
    "\n",
    "## Binning for Cintinuous and categorical \n",
    "\n",
    "def assign_win_bins(W):\n",
    "    if W < 50:                    # Creating a bin value of 1 for wins amounting less than 50\n",
    "        return(1)\n",
    "    if W >= 50 and W <= 69:       # Creating a bin value of 2 for wins amounting less than 70 and greater than 50 \n",
    "        return(2)\n",
    "    if W >= 70 and W <= 89:       # Creating a bin value of 3 for wins amounting less than 90 and greater than 70\n",
    "        return(3)\n",
    "    if W >= 90 and W <= 109:      # Creating a bin value of 4 for wins amounting less than 110 and greater than 90\n",
    "        return(4)\n",
    "    if W >= 110:                  # Creating a bin value of 5 for wins amounting greater than 110\n",
    "        return(5)\n",
    "   \n",
    "df['Win_bins'] = df['W'].apply(assign_win_bins)  # Creating a new column Win_bins in the dataframe\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably already know that we need to look at the newly binned column (`Win_bins`). Instead of bar chart (distribution chart), we want to look at its values crossed by `Years`.\n",
    "\n",
    "Scatterplot is very suitable for that purpose.\n",
    "\n",
    "You will use the `scatter()` method to create the scatterplot. In the `scatter()` method: \n",
    "- __STEP1__: you need to first define two axis: `df['yearID']` and `df['W']`. __Note__ that we need the continuous value for the axis so that we use `df['W']` instead of `df['Win_bins]`;\n",
    "- __STEP2__: you need to define how to color your points by invoking the `c=` parameter. Here we want to color the data points by `df['Win_bins']`;\n",
    "- __STEP3__: As said above, we want to add title (`Win by Year Scatterplot`), x-axis label (`Years`), and y-axis label (`Wins`) to the scatterplot. __Hint__: as we did before, you can use `plt.title()` method to add title to your plot;\n",
    "- __STEP4__: Then you need to display your plot by calling the `show()` method.\n",
    "\n",
    "Use above logic to complete the code block below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Scatter plot for the yearwise wins\n",
    "\n",
    "#### STEP 1 & 2\n",
    "plt.scatter(#####)\n",
    "\n",
    "#### STEP 3\n",
    "\n",
    "#### STEP 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our binning is good - bins `[2, 3, 4]` capture the majority of data, while bins `[1, 5]` capture the extreme values (outliers).\n",
    "\n",
    "Let's save the processed data to a CSV file so that we can re-use it in part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('../data/baseball_analytics_pt1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all for part one. Please make sure your sync the complete notebook to your github repo for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
